<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>基于MNIST的ConvNetJS训练器对比</title>
  <meta name="description" content="">
  <meta name="author" content="">
  <link rel="stylesheet" href="css/style.css">

<script src="js/jquery-1.8.3.min.js"></script>
<script src="../build/vis.js"></script>
<script src="../build/util.js"></script>
<script src="../build/convnet.js"></script>
<script src="mnist/mnist_labels.js"></script>
<script src="js/trainers.js"></script>

</head>
<body>
  <div id="wrap">
  <h2 style="text-align: center;">基于MNIST的<a href="http://cs.stanford.edu/people/karpathy/convnetjs/">ConvNetJS</a>训练器演示</h2>
  <h1>描述</h1>
  <p>
    此演示允许您在MNIST上相互评估多个培训器。 默认情况下设置了一个小基准，让SGD / 动量SGD / Adam / Adagrad / Adadelta / Nesterov相互对抗。 有关这些的参考数学和解释，请参考Matthew Zeiler的<a href="http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf">Adadelta论文</a>（Windowgrad是论文中的想法＃1）。 经验上，Adagrad / Adadelta“更安全”，因为他们并没有如此强烈地依赖于学习率的设定（Adadelta稍好一些），但是经过良好调整的SGD + Momentum几乎总是收敛得更快，并且具有更好的最终价值。
  </p>
  <p>向<a href="https://weibo.com/gduflabs">@gduflabs</a>报告问题/错误/建议.</p>

  <textarea id="layerdef" style="width:100%; height:400px;">
  </textarea>
  <br /><br  />
  <input id="buttontp" type="submit" value="re-run" onclick="reload();" style="width: 300px; height: 50px;"/>

  <h1>损失vs样本数量</h1>
  <canvas id="lossgraph" width="800" height="400"></canvas>

  <h1>测试准确度vs样本数量</h1>
  <canvas id="testaccgraph" width="800" height="400"></canvas>

  <h1>训练准确性vs样本数量</h1>
  <canvas id="trainaccgraph" width="800" height="400"></canvas>

  </div>
</body>
</html>



